{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e74c1-71e2-40ff-91b8-fdb271c8bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Silence RDKit chatter\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "\n",
    "class JSONLPotencyDataset(Dataset):\n",
    "    def __init__(self, path_to_jsonl_gz):\n",
    "        self.records = []\n",
    "        with gzip.open(path_to_jsonl_gz, \"rt\") as f:\n",
    "            for line in f:\n",
    "                rec = json.loads(line)\n",
    "                self.records.append((rec[\"smiles\"], rec[\"label_vector\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles, label_vec = self.records[idx]\n",
    "        fp = fp6144_from_smiles(smiles)\n",
    "        while fp is None:\n",
    "            idx = (idx + 1) % len(self.records)\n",
    "            smiles, label_vec = self.records[idx]\n",
    "            fp = fp6144_from_smiles(smiles)\n",
    "        return fp, torch.tensor(label_vec, dtype=torch.long)\n",
    "\n",
    "# Fingerprint helper (NOT NEW fingerprint method)\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "GEN0 = AllChem.GetMorganGenerator(radius=0, fpSize=2048)\n",
    "GEN1 = AllChem.GetMorganGenerator(radius=1, fpSize=2048)\n",
    "GEN2 = AllChem.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def fp6144_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp0 = torch.tensor(list(GEN0.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp1 = torch.tensor(list(GEN1.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp2 = torch.tensor(list(GEN2.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    return torch.cat([fp0, fp1, fp2])  # (6144,)\n",
    "\n",
    "# Model definition \n",
    "class MultiLineMLP5(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=6144,\n",
    "                 hidden_dims=[1024, 1024, 512, 512, 256],\n",
    "                 num_lines=60,\n",
    "                 num_classes=6,\n",
    "                 p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_in = nn.BatchNorm1d(input_dim)\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for d_in, d_out in zip(dims[:-1], dims[1:]):\n",
    "            layers += [\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.BatchNorm1d(d_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p_drop),\n",
    "            ]\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(hidden_dims[-1], num_lines * num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn_in(x)\n",
    "        feat = self.shared(x)\n",
    "        logits = self.classifier(feat)\n",
    "        return logits.view(-1, 60, 6)\n",
    "        \n",
    "train_ds = JSONLPotencyDataset(\"train_resampled.jsonl.gz\")\n",
    "val_ds   = JSONLPotencyDataset(\"val.jsonl.gz\")\n",
    "test_ds  = JSONLPotencyDataset(\"test.jsonl.gz\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64,\n",
    "                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64,\n",
    "                          shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64,\n",
    "                          shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Training setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = MultiLineMLP5().to(device)\n",
    "\n",
    "# No class-weights here:\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "def full_val_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = np.zeros(60); total = np.zeros(60)\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x).argmax(2)\n",
    "            mask = y != -1\n",
    "            correct += ((pred == y) & mask).sum(0).cpu().numpy()\n",
    "            total   += mask.sum(0).cpu().numpy()\n",
    "    accs = [c / t if t>0 else 0.0 for c, t in zip(correct, total)]\n",
    "    return float(np.nanmean(accs)), accs\n",
    "\n",
    "# --- Training loop ---\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, 41):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1,6), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            val_loss += criterion(\n",
    "                model(x).view(-1,6), y.view(-1)\n",
    "            ).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    avg_acc, _ = full_val_accuracy(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch:02} | Train Loss {train_loss:.1f} \"\n",
    "          f\"| Val Loss {val_loss:.1f} | Acc {avg_acc:.4f} \"\n",
    "          f\"| LR {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        torch.save(model.state_dict(), \"best_resampled.pt\")\n",
    "        print(f\"  âœ“ New best: {best_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete. Best Avg Val Acc =\", best_acc)\n",
    "http://localhost:9993/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/tqdm/auto.py#line=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0fe34-5246-400d-95c5-7dc739eefa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Define the function (one cell)\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_precision_recall(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(dim=2)\n",
    "            mask = (y != -1)\n",
    "            all_preds.append( preds[mask].cpu().numpy().ravel() )\n",
    "            all_labels.append(y[mask].cpu().numpy().ravel())\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    precision = np.zeros(6); recall = np.zeros(6)\n",
    "    for c in range(6):\n",
    "        tp = ((y_pred==c)&(y_true==c)).sum()\n",
    "        fp = ((y_pred==c)&(y_true!=c)).sum()\n",
    "        fn = ((y_pred!=c)&(y_true==c)).sum()\n",
    "        precision[c] = tp/(tp+fp) if tp+fp>0 else np.nan\n",
    "        recall[c]    = tp/(tp+fn) if tp+fn>0 else np.nan\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be68e33-7790-4e43-9c59-a698ff34063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Call it in a fresh cell once training is done\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"best_resampled.pt\"))  # pick model\n",
    "model.to(device)\n",
    "\n",
    "precision, recall = compute_precision_recall(model, val_loader, device)\n",
    "for c in range(6):\n",
    "    print(f\"class {c}:  prec={precision[c]:.3f},  rec={recall[c]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324dca82-c32a-4d29-ba6e-260a7a2744a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def macro_precision_recall(model, loader, device, num_lines=59):\n",
    "    \"\"\"\n",
    "    Returns two 2-D arrays of shape (6 classes, 2 metrics):\n",
    "        [exact , within-one] for precision and recall.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # per-line confusion buckets\n",
    "    exact = defaultdict(lambda: np.zeros((6, 3), dtype=int))       # TP, FP, FN\n",
    "    within = defaultdict(lambda: np.zeros((6, 3), dtype=int))      # TPw, FPw, FNw\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:                         # y: (B, 60)\n",
    "            x = x.to(device)\n",
    "            logits = model(x)[:, :num_lines]        # (B, 59, 6)  drop 60th line\n",
    "            preds  = logits.argmax(2).cpu().numpy()\n",
    "            y      = y[:, :num_lines].cpu().numpy()\n",
    "            \n",
    "            for line_idx in range(num_lines):\n",
    "                true_line   = y[:, line_idx]\n",
    "                pred_line   = preds[:, line_idx]\n",
    "                mask        = true_line != -1\n",
    "                yt          = true_line[mask]\n",
    "                yp          = pred_line[mask]\n",
    "                if yt.size == 0:                    # line had no labels\n",
    "                    continue\n",
    "                \n",
    "                for c in range(6):\n",
    "                    tp = np.sum((yp == c) & (yt == c))\n",
    "                    fp = np.sum((yp == c) & (yt != c))\n",
    "                    fn = np.sum((yp != c) & (yt == c))\n",
    "                    exact[line_idx][c] += (tp, fp, fn)\n",
    "                    \n",
    "                    # within-one: hit if |pred-true| <= 1\n",
    "                    tp_w = np.sum((yp == c) & (np.abs(yt - c) <= 1))\n",
    "                    fp_w = np.sum((yp == c) & (np.abs(yt - c) >  1))\n",
    "                    fn_w = np.sum((yp != c) & (np.abs(yp - c) <= 1) & (yt == c))\n",
    "                    within[line_idx][c] += (tp_w, fp_w, fn_w)\n",
    "    \n",
    "    # macro-average over cell lines (skip lines with zero support)\n",
    "    prec = np.zeros((6, 2))\n",
    "    rec  = np.zeros((6, 2))\n",
    "    for c in range(6):\n",
    "        # gather per-line metrics then average\n",
    "        p_exact, r_exact, p_within, r_within = [], [], [], []\n",
    "        for line in exact.keys():\n",
    "            tp, fp, fn         = exact[line][c]\n",
    "            tpw, fpw, fnw      = within[line][c]\n",
    "            if tp + fn == 0:    # no true instances of class c in this line\n",
    "                continue\n",
    "            p_exact.append(  tp / (tp + fp) if tp+fp>0 else np.nan )\n",
    "            r_exact.append(  tp / (tp + fn)                 )\n",
    "            p_within.append( tpw / (tpw + fpw) if tpw+fpw>0 else np.nan )\n",
    "            r_within.append( tpw / (tpw + fnw)               )\n",
    "        prec[c, 0] = np.nanmean(p_exact)\n",
    "        prec[c, 1] = np.nanmean(p_within)\n",
    "        rec[c, 0]  = np.nanmean(r_exact)\n",
    "        rec[c, 1]  = np.nanmean(r_within)\n",
    "    return prec, rec\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example usage on your validation set\n",
    "prec, rec = macro_precision_recall(model, val_loader, device)\n",
    "\n",
    "headers = [\"Exact P\", \"Â±1 P\", \"Exact R\", \"Â±1 R\"]\n",
    "print(f\"{'Class':>6}  {headers[0]:>8}  {headers[1]:>8}  \"\n",
    "      f\"{headers[2]:>8}  {headers[3]:>8}\")\n",
    "for c in range(6):\n",
    "    print(f\"{c:>6}  {prec[c,0]*100:8.1f}%  {prec[c,1]*100:8.1f}%  \"\n",
    "          f\"{rec[c,0]*100:8.1f}%  {rec[c,1]*100:8.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be254f8f-a4d0-4a98-9f5d-660cf902ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precision_recall(model, loader, device, num_lines=59):\n",
    "    \"\"\"\n",
    "    Micro-averaged precision and recall across all cell lines and samples.\n",
    "    Returns: (precision_exact, recall_exact), (precision_within1, recall_within1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x)[:, :num_lines]     # (B, 59, 6)\n",
    "            preds = logits.argmax(dim=2).cpu().numpy()   # (B, 59)\n",
    "            y     = y[:, :num_lines].cpu().numpy()        # (B, 59)\n",
    "\n",
    "            mask = (y != -1)\n",
    "            y_true_all.append(y[mask])\n",
    "            y_pred_all.append(preds[mask])\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    precision = []\n",
    "    recall    = []\n",
    "    precision1 = []\n",
    "    recall1    = []\n",
    "\n",
    "    for c in range(6):\n",
    "        tp  = np.sum((y_pred == c) & (y_true == c))\n",
    "        fp  = np.sum((y_pred == c) & (y_true != c))\n",
    "        fn  = np.sum((y_pred != c) & (y_true == c))\n",
    "\n",
    "        tp1 = np.sum((y_pred == c) & (np.abs(y_true - c) <= 1))\n",
    "        fp1 = np.sum((y_pred == c) & (np.abs(y_true - c) >  1))\n",
    "        fn1 = np.sum((y_pred != c) & (y_true == c) & (np.abs(y_pred - c) <= 1))\n",
    "\n",
    "        p  = tp  / (tp + fp) if (tp + fp) > 0 else np.nan\n",
    "        r  = tp  / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        p1 = tp1 / (tp1 + fp1) if (tp1 + fp1) > 0 else np.nan\n",
    "        r1 = tp1 / (tp1 + fn1) if (tp1 + fn1) > 0 else np.nan\n",
    "\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        precision1.append(p1)\n",
    "        recall1.append(r1)\n",
    "\n",
    "    return np.array(precision), np.array(recall), np.array(precision1), np.array(recall1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2f290-23fc-4967-8a71-1285fb56187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, prec1, rec1 = micro_precision_recall(model, val_loader, device)\n",
    "\n",
    "headers = [\"Exact P\", \"Â±1 P\", \"Exact R\", \"Â±1 R\"]\n",
    "print(f\"{'Class':>6}  {headers[0]:>8}  {headers[1]:>8}  \"\n",
    "      f\"{headers[2]:>8}  {headers[3]:>8}\")\n",
    "for c in range(6):\n",
    "    print(f\"{c:>6}  {prec[c]*100:8.1f}%  {prec1[c]*100:8.1f}%  \"\n",
    "          f\"{rec[c]*100:8.1f}%  {rec1[c]*100:8.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54720c-5370-4960-8bcb-4e34749a4363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
