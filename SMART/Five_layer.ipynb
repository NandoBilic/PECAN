{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f28d5f5-6737-48ba-bbe5-842ecbe6899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json, torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "# -- Morgan‑FP helpers (unchanged) -----------------------------------------\n",
    "GEN0 = AllChem.GetMorganGenerator(radius=0, fpSize=2048)\n",
    "GEN1 = AllChem.GetMorganGenerator(radius=1, fpSize=2048)\n",
    "GEN2 = AllChem.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def fp6144_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp0 = torch.tensor(list(GEN0.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp1 = torch.tensor(list(GEN1.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp2 = torch.tensor(list(GEN2.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    return torch.cat([fp0, fp1, fp2])        # shape (6144,)\n",
    "\n",
    "# -- NEW dataset class -----------------------------------------------------\n",
    "class JSONLPotencyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the new *.jsonl.gz split files created in the parsing notebook.\n",
    "    Each line contains {\"smiles\": ..., \"label_vector\": [...] }.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_jsonl_gz):\n",
    "        with gzip.open(path_to_jsonl_gz, \"rt\") as f:\n",
    "            self.records = [(rec[\"smiles\"], rec[\"label_vector\"])\n",
    "                            for rec in (json.loads(l) for l in f)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles, label_vec = self.records[idx]\n",
    "        fp = fp6144_from_smiles(smiles)\n",
    "\n",
    "        # fall‑through if RDKit fails on this SMILES\n",
    "        while fp is None:\n",
    "            idx = (idx + 1) % len(self.records)\n",
    "            smiles, label_vec = self.records[idx]\n",
    "            fp = fp6144_from_smiles(smiles)\n",
    "\n",
    "        labels = torch.tensor(label_vec, dtype=torch.long)   # (60,)\n",
    "        return fp, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27f07af-b9dd-4cd1-98ba-8da0f0dec550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLineMLP5(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=6144,\n",
    "                 hidden_dims=[1024, 1024, 512, 512, 256],\n",
    "                 num_lines=60,\n",
    "                 num_classes=6,\n",
    "                 p_drop=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn_in = nn.BatchNorm1d(input_dim)\n",
    "\n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for d_in, d_out in zip(dims[:-1], dims[1:]):\n",
    "            layers.extend([\n",
    "                nn.Linear(d_in, d_out),\n",
    "                nn.BatchNorm1d(d_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p_drop)\n",
    "            ])\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dims[-1], num_lines * num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn_in(x)\n",
    "        x = self.shared(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits.view(-1, 60, 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "332efd46-d7d7-4f22-b518-4dbf01dc2372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Silence RDKit chatter\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "# ---------- DATA ----------------------------------------------------------\n",
    "train_dataset = JSONLPotencyDataset(\"train.jsonl.gz\")\n",
    "val_dataset   = JSONLPotencyDataset(\"val.jsonl.gz\")\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64,\n",
    "                           shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=64,\n",
    "                           num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---------- MODEL ---------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiLineMLP5().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa57f33-4e34-4a91-850f-c3a7354452c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/40 | Train Loss: 1082.8 | Val Loss: 158.7 | Avg Val Acc: 0.3560 | LR: 5.0e-04\n",
      "  ✓ saved new best (0.3560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02/40 | Train Loss: 913.2 | Val Loss: 171.2 | Avg Val Acc: 0.3709 | LR: 5.0e-04\n",
      "  ✓ saved new best (0.3709)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03/40 | Train Loss: 844.2 | Val Loss: 184.7 | Avg Val Acc: 0.3666 | LR: 5.0e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04/40 | Train Loss: 806.9 | Val Loss: 173.7 | Avg Val Acc: 0.3578 | LR: 2.5e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05/40 | Train Loss: 727.3 | Val Loss: 195.9 | Avg Val Acc: 0.3676 | LR: 2.5e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06/40 | Train Loss: 697.2 | Val Loss: 194.4 | Avg Val Acc: 0.3735 | LR: 2.5e-04\n",
      "  ✓ saved new best (0.3735)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07/40 | Train Loss: 676.0 | Val Loss: 190.0 | Avg Val Acc: 0.3504 | LR: 1.3e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08/40 | Train Loss: 630.7 | Val Loss: 198.6 | Avg Val Acc: 0.3663 | LR: 1.3e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09/40 | Train Loss: 609.4 | Val Loss: 184.7 | Avg Val Acc: 0.4004 | LR: 1.3e-04\n",
      "  ✓ saved new best (0.4004)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 | Train Loss: 596.5 | Val Loss: 191.0 | Avg Val Acc: 0.3908 | LR: 6.3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 | Train Loss: 573.1 | Val Loss: 195.9 | Avg Val Acc: 0.3993 | LR: 6.3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 | Train Loss: 563.2 | Val Loss: 206.4 | Avg Val Acc: 0.3773 | LR: 6.3e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 | Train Loss: 553.6 | Val Loss: 199.2 | Avg Val Acc: 0.3936 | LR: 3.1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 | Train Loss: 542.3 | Val Loss: 195.8 | Avg Val Acc: 0.3957 | LR: 3.1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 | Train Loss: 538.5 | Val Loss: 204.9 | Avg Val Acc: 0.3939 | LR: 3.1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 | Train Loss: 532.0 | Val Loss: 190.3 | Avg Val Acc: 0.4060 | LR: 1.6e-05\n",
      "  ✓ saved new best (0.4060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 | Train Loss: 526.8 | Val Loss: 199.5 | Avg Val Acc: 0.3907 | LR: 1.6e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 | Train Loss: 523.8 | Val Loss: 193.4 | Avg Val Acc: 0.4123 | LR: 1.6e-05\n",
      "  ✓ saved new best (0.4123)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 | Train Loss: 523.5 | Val Loss: 197.1 | Avg Val Acc: 0.4083 | LR: 7.8e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 | Train Loss: 519.6 | Val Loss: 202.9 | Avg Val Acc: 0.4047 | LR: 7.8e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train Loss: 516.9 | Val Loss: 186.5 | Avg Val Acc: 0.4226 | LR: 7.8e-06\n",
      "  ✓ saved new best (0.4226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train Loss: 518.2 | Val Loss: 194.7 | Avg Val Acc: 0.4059 | LR: 3.9e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train Loss: 515.1 | Val Loss: 192.1 | Avg Val Acc: 0.4229 | LR: 3.9e-06\n",
      "  ✓ saved new best (0.4229)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train Loss: 514.6 | Val Loss: 192.8 | Avg Val Acc: 0.4235 | LR: 3.9e-06\n",
      "  ✓ saved new best (0.4235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train Loss: 512.2 | Val Loss: 185.2 | Avg Val Acc: 0.4129 | LR: 2.0e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train Loss: 513.6 | Val Loss: 199.2 | Avg Val Acc: 0.4135 | LR: 2.0e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train Loss: 514.1 | Val Loss: 191.1 | Avg Val Acc: 0.4164 | LR: 2.0e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train Loss: 513.3 | Val Loss: 194.2 | Avg Val Acc: 0.4194 | LR: 9.8e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train Loss: 511.5 | Val Loss: 198.4 | Avg Val Acc: 0.3885 | LR: 9.8e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train Loss: 512.9 | Val Loss: 196.4 | Avg Val Acc: 0.4184 | LR: 9.8e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train Loss: 511.9 | Val Loss: 206.3 | Avg Val Acc: 0.4170 | LR: 4.9e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train Loss: 511.3 | Val Loss: 189.8 | Avg Val Acc: 0.4167 | LR: 4.9e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-136:██████████████████▋       | 603/730 [00:42<00:07, 17.77it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/multiprocessing/reductions.py\", line 495, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "                                                                                           "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 45\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss   \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m6\u001b[39m), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m, in \u001b[0;36mMultiLineMLP5.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshared(x)\n\u001b[1;32m     28\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:154\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    150\u001b[0m     exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Nandos/lib/python3.9/site-packages/torch/nn/modules/module.py:1698\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m-> 1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_parameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1699\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _parameters:\n\u001b[1;32m   1700\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m _parameters[name]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------- 1.  class‑imbalance weights  ----------------------------------\n",
    "hist = torch.zeros(6)\n",
    "for _, labels in train_loader:\n",
    "    mask = labels != -1\n",
    "    for c in range(6):\n",
    "        hist[c] += ((labels == c) & mask).sum()\n",
    "\n",
    "weights = 1.0 / (hist + 1e-6)\n",
    "weights = (weights / weights.sum()) * 6\n",
    "weights = weights.to(torch.float32).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, weight=weights)\n",
    "\n",
    "# ---------- 2.  full‑set validation accuracy ------------------------------\n",
    "def full_val_accuracy(model, loader, device):\n",
    "    correct = np.zeros(60); total = np.zeros(60)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            p    = model(x).argmax(2)\n",
    "            m    = y != -1\n",
    "            correct += ((p == y) & m).sum(0).cpu().numpy()\n",
    "            total   += m.sum(0).cpu().numpy()\n",
    "    accs = [c / t if t > 0 else None for c, t in zip(correct, total)]\n",
    "    return float(np.nanmean(accs)), accs\n",
    "\n",
    "# ---------- 3.  optimiser, scheduler, loop --------------------------------\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "best_acc = 0.0\n",
    "num_epochs = 40\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss   = criterion(logits.view(-1, 6), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # VALIDATE\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            val_loss += criterion(\n",
    "                model(x).view(-1, 6), y.view(-1)\n",
    "            ).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    avg_acc, _ = full_val_accuracy(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch:02}/{num_epochs} | \"\n",
    "          f\"Train Loss: {epoch_loss:.1f} | Val Loss: {val_loss:.1f} | \"\n",
    "          f\"Avg Val Acc: {avg_acc:.4f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"  ✓ saved new best ({best_acc:.4f})\")\n",
    "\n",
    "print(f\"Done. Best Avg Val Acc = {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22929cb8-26db-4687-9b2e-16f86ae186f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
