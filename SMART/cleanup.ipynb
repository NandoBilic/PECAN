{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d8e22d3-6279-4be8-9711-2da54cba4147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbilic/miniconda3/envs/Nandos/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports and Setup\n",
    "import gzip, json, torch, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5847e0f-4776-431e-a8c4-c22ab1513010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Morgan Fingerprint Helper\n",
    "GEN0 = AllChem.GetMorganGenerator(radius=0, fpSize=2048)\n",
    "GEN1 = AllChem.GetMorganGenerator(radius=1, fpSize=2048)\n",
    "GEN2 = AllChem.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def fp6144_from_smiles(s):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None: return None\n",
    "    f0 = torch.tensor(list(GEN0.GetFingerprint(m)),dtype=torch.float32)\n",
    "    f1 = torch.tensor(list(GEN1.GetFingerprint(m)),dtype=torch.float32)\n",
    "    f2 = torch.tensor(list(GEN2.GetFingerprint(m)),dtype=torch.float32)\n",
    "    return torch.cat([f0,f1,f2])  # (6144,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f4b4f7-ac4d-44e3-b4c3-4f5dbc6da36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset Class\n",
    "class JSONLPotencyDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.records = []\n",
    "        with gzip.open(path,'rt') as f:\n",
    "            for line in f:\n",
    "                r = json.loads(line)\n",
    "                self.records.append((r['smiles'],r['label_vector']))\n",
    "    def __len__(self): return len(self.records)\n",
    "    def __getitem__(self,i):\n",
    "        s, lbl = self.records[i]\n",
    "        fp = fp6144_from_smiles(s)\n",
    "        while fp is None:\n",
    "            i = (i+1)%len(self)\n",
    "            s, lbl = self.records[i]\n",
    "            fp = fp6144_from_smiles(s)\n",
    "        return fp, torch.tensor(lbl,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82fc68c-9399-4804-942b-71243fa03693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Definition\n",
    "class MultiLineMLP(nn.Module):\n",
    "    def __init__(self, input_dim=6144, hidden_dims=[1024,1024,512,512,256],\n",
    "                 num_lines=60, num_classes=6, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(input_dim)\n",
    "        layers=[]\n",
    "        dims=[input_dim]+hidden_dims\n",
    "        for a,b in zip(dims,dims[1:]):\n",
    "            layers += [nn.Linear(a,b), nn.BatchNorm1d(b), nn.ReLU(), nn.Dropout(p_drop)]\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(hidden_dims[-1], num_lines*num_classes)\n",
    "    def forward(self,x):\n",
    "        x=self.bn(x); f=self.shared(x)\n",
    "        out=self.classifier(f)\n",
    "        return out.view(out.size(0),-1,6)\n",
    "\n",
    "# 4b. Training Options\n",
    "# You can choose model depth, sampling strategy, and whether to use weighted loss.\n",
    "MODEL_CONFIGS = {\n",
    "    '5layer': [1024, 1024, 512, 512, 256]\n",
    "}\n",
    "MODEL_CONFIG = MODEL_CONFIGS['5layer']  # always 5-layer\n",
    "SAMPLING_METHOD = 'weighted_sampler'  # options: 'none', 'resampled', 'weighted_sampler'\n",
    "WEIGHTED_LOSS   = True     # True to use class weights in loss, False otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3ea590c-4cb1-43cf-b910-f289994da1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch loaded: torch.Size([64, 6144]) torch.Size([64, 60])\n"
     ]
    }
   ],
   "source": [
    "# 5. Configuration\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Silence RDKit chatter\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "TRAIN_JSON = 'train.jsonl.gz'\n",
    "VAL_JSON   = 'val.jsonl.gz'\n",
    "BATCH      = 64\n",
    "LR         = 5e-4\n",
    "WD         = 1e-4\n",
    "EPOCHS     = 20\n",
    "DROP       = 0.3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 6. Instantiate Dataset and DataLoader with Sampling\n",
    "train_ds = JSONLPotencyDataset(TRAIN_JSON)\n",
    "val_ds   = JSONLPotencyDataset(VAL_JSON)\n",
    "\n",
    "if SAMPLING_METHOD == 'resampled':\n",
    "    # compute sample weights inversely proportional to class freq\n",
    "    all_labels = torch.cat([lbl for _,lbl in DataLoader(train_ds, batch_size=BATCH)])\n",
    "    mask = all_labels != -1\n",
    "    freq = torch.bincount(all_labels[mask], minlength=6).float()\n",
    "    inv = 1.0 / (freq + 1e-6)\n",
    "    sample_weights = []\n",
    "    for _, lbl in train_ds:\n",
    "        w = inv[lbl[lbl>=0]].mean().item()\n",
    "        sample_weights.append(w)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler)\n",
    "elif SAMPLING_METHOD == 'weighted_sampler':\n",
    "    # use sqrt-inverse weighting\n",
    "    all_labels = torch.cat([lbl for _,lbl in DataLoader(train_ds, batch_size=BATCH)])\n",
    "    mask = all_labels != -1\n",
    "    freq = torch.bincount(all_labels[mask], minlength=6).float()\n",
    "    inv_sqrt = 1.0 / torch.sqrt(freq + 1e-6)\n",
    "    inv_sqrt /= inv_sqrt.mean()\n",
    "    sample_weights = []\n",
    "    for _, lbl in train_ds:\n",
    "        w = inv_sqrt[lbl[lbl>=0]].mean().item()\n",
    "        sample_weights.append(w)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH, sampler=sampler)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH)\n",
    "\n",
    "# 7. Loss & Optimizer Setup\n",
    "# instantiate model\n",
    "dims = MODEL_CONFIG\n",
    "model = MultiLineMLP(hidden_dims=dims, p_drop=DROP).to(device)\n",
    "\n",
    "# criterion\n",
    "if WEIGHTED_LOSS:\n",
    "    # compute class weights from training loader\n",
    "    all_labels = torch.cat([lbl for _,lbl in DataLoader(train_ds, batch_size=BATCH)])\n",
    "    mask = all_labels != -1\n",
    "    freq = torch.bincount(all_labels[mask], minlength=6).float()\n",
    "    class_weights = (1.0 / (freq + 1e-6))\n",
    "    class_weights = (class_weights / class_weights.sum()) * 6.0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1, weight=class_weights.to(device))\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "# 8. Quick Test\n",
    "test_x, test_y = next(iter(train_loader))\n",
    "print('Test batch loaded:', test_x.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0735e8-2a9b-43c4-8f37-ac0f33dec9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████| 730/730 [02:37<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.6284 | Val Acc 0.4178\n",
      "  ✓ New best: 0.4178 saved to best_weighted_sampler_weighted.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████| 730/730 [02:35<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.6119 | Val Acc 0.4547\n",
      "  ✓ New best: 0.4547 saved to best_weighted_sampler_weighted.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████| 730/730 [02:35<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.6179 | Val Acc 0.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|███████████████████████████████████████████████| 730/730 [02:34<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.6107 | Val Acc 0.4003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████| 730/730 [02:38<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.6114 | Val Acc 0.4288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|███████████████████████████████████████████████| 730/730 [02:35<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Loss 0.6123 | Val Acc 0.4067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|███████████████████████████████████████████████| 730/730 [02:37<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Loss 0.6089 | Val Acc 0.4217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|███████████████████████████████████████████████| 730/730 [02:37<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss 0.6100 | Val Acc 0.4319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|███████████████████████████████████████████████| 730/730 [02:36<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss 0.6019 | Val Acc 0.4422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████████████████████████████████████████| 730/730 [02:36<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Loss 0.5949 | Val Acc 0.4074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████████████████████████████████████████| 730/730 [02:36<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Loss 0.5981 | Val Acc 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████████████████████████████████████████| 730/730 [02:38<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Loss 0.5990 | Val Acc 0.4328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████████████████████████████████████████| 730/730 [02:35<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Loss 0.6062 | Val Acc 0.4140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████████████████████████████████████████| 730/730 [02:36<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Loss 0.5909 | Val Acc 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15:  42%|███████████████████▎                          | 307/730 [01:05<01:29,  4.75it/s]"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    model.train(); tloss=0.0\n",
    "    for x,y in tqdm(train_loader,desc=f'Epoch {epoch}'):\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.view(-1,6), y.view(-1))\n",
    "        loss.backward(); optimizer.step()\n",
    "        tloss += loss.item()\n",
    "    avg_loss = tloss/len(train_loader)\n",
    "    model.eval(); correct=0; total=0\n",
    "    with torch.no_grad():\n",
    "        for vx,vy in val_loader:\n",
    "            vx,vy = vx.to(device), vy.to(device)\n",
    "            p = model(vx).argmax(2); m = (vy!=-1)\n",
    "            correct += ((p==vy)&m).sum().item()\n",
    "            total   += m.sum().item()\n",
    "    val_acc = correct/total if total>0 else 0.0\n",
    "    print(f'Epoch {epoch} | Loss {avg_loss:.4f} | Val Acc {val_acc:.4f}')\n",
    "    if val_acc>best_acc:\n",
    "        best_acc=val_acc\n",
    "        ckpt = f\"best_{SAMPLING_METHOD}_{'weighted' if WEIGHTED_LOSS else 'unweighted'}.pt\"\n",
    "        torch.save(model.state_dict(),ckpt)\n",
    "        print(f'  ✓ New best: {best_acc:.4f} saved to {ckpt}')\n",
    "print(f'Training complete. Best Val Acc = {best_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed52fc54-a456-4e6d-abc2-940eeab19a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3871216230529495\n"
     ]
    }
   ],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval(); correct=0; total=0\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y=x.to(device),y.to(device)\n",
    "            preds=model(x).argmax(2)\n",
    "            mask=y!=-1\n",
    "            correct += ((preds==y)&mask).sum().item()\n",
    "            total   += mask.sum().item()\n",
    "    return correct/total\n",
    "\n",
    "acc=evaluate(val_loader)\n",
    "print('Validation accuracy:',acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
