{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77865e8-798a-4017-820e-eb4d3b82b35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json, torch\n",
    "from torch.utils.data import Dataset\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "GEN0 = AllChem.GetMorganGenerator(radius=0, fpSize=2048)\n",
    "GEN1 = AllChem.GetMorganGenerator(radius=1, fpSize=2048)\n",
    "GEN2 = AllChem.GetMorganGenerator(radius=2, fpSize=2048)\n",
    "\n",
    "def fp6144_from_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    fp0 = torch.tensor(list(GEN0.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp1 = torch.tensor(list(GEN1.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    fp2 = torch.tensor(list(GEN2.GetFingerprint(mol)), dtype=torch.float32)\n",
    "    return torch.cat([fp0, fp1, fp2])        # shape (6144,)\n",
    "\n",
    "class JSONLPotencyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Loads the new *.jsonl.gz split files created in the parsing notebook.\n",
    "    Each line contains {\"smiles\": ..., \"label_vector\": [...] }.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_to_jsonl_gz):\n",
    "        with gzip.open(path_to_jsonl_gz, \"rt\") as f:\n",
    "            self.records = [(rec[\"smiles\"], rec[\"label_vector\"])\n",
    "                            for rec in (json.loads(l) for l in f)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles, label_vec = self.records[idx]\n",
    "        fp = fp6144_from_smiles(smiles)\n",
    "\n",
    "        # fall‑through if RDKit fails on this SMILES\n",
    "        while fp is None:\n",
    "            idx = (idx + 1) % len(self.records)\n",
    "            smiles, label_vec = self.records[idx]\n",
    "            fp = fp6144_from_smiles(smiles)\n",
    "\n",
    "        labels = torch.tensor(label_vec, dtype=torch.long)   # (60,)\n",
    "        return fp, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81319e-6dad-420c-8602-4a24bdc794e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiLineMLP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim=6144,\n",
    "                 hidden_dim=768,     # ↑ from 256\n",
    "                 num_lines=60,\n",
    "                 num_classes=6):\n",
    "        super().__init__()\n",
    "        self.bn_in = nn.BatchNorm1d(input_dim)           # NEW\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.6),\n",
    "        )\n",
    "\n",
    "        # one big classifier → (batch, 60, 6)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_lines * num_classes)\n",
    "\n",
    "    def forward(self, x):                                # x: (batch, 6144)\n",
    "        x = self.bn_in(x)\n",
    "        feat = self.shared(x)                            # (batch, 768)\n",
    "        logits = self.classifier(feat)                   # (batch, 360)\n",
    "        return logits.view(-1, 60, 6)                    # (batch, 60, 6)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57220f02-e099-43f6-a2bd-517221a38451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Silence RDKit chatter\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "\n",
    "train_dataset = JSONLPotencyDataset(\"train.jsonl.gz\")\n",
    "val_dataset   = JSONLPotencyDataset(\"val.jsonl.gz\")\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=64,\n",
    "                           shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=64,\n",
    "                           num_workers=4, pin_memory=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MultiLineMLP().to(device)   # the new BN + shared‑head version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e3631-e562-41a6-9d53-c5f04735ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING cell \n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class‑imbalance weights\n",
    "hist = torch.zeros(6)\n",
    "for _, labels in train_loader:\n",
    "    mask = labels != -1\n",
    "    for c in range(6):\n",
    "        hist[c] += ((labels == c) & mask).sum()\n",
    "\n",
    "weights = 1.0 / (hist + 1e-6)\n",
    "weights = (weights / weights.sum()) * 6\n",
    "weights = weights.to(torch.float32).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1, weight=weights)\n",
    "\n",
    "# full‑set validation accuracy \n",
    "def full_val_accuracy(model, loader, device):\n",
    "    correct = np.zeros(60); total = np.zeros(60)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            p    = model(x).argmax(2)\n",
    "            m    = y != -1\n",
    "            correct += ((p == y) & m).sum(0).cpu().numpy()\n",
    "            total   += m.sum(0).cpu().numpy()\n",
    "    accs = [c / t if t > 0 else None for c, t in zip(correct, total)]\n",
    "    return float(np.nanmean(accs)), accs\n",
    "\n",
    "#  optimiser, scheduler, loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "best_acc = 0.0\n",
    "num_epochs = 40\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # TRAIN\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss   = criterion(logits.view(-1, 6), y.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # VALIDATE\n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            val_loss += criterion(\n",
    "                model(x).view(-1, 6), y.view(-1)\n",
    "            ).item()\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    avg_acc, _ = full_val_accuracy(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch:02}/{num_epochs} | \"\n",
    "          f\"Train Loss: {epoch_loss:.1f} | Val Loss: {val_loss:.1f} | \"\n",
    "          f\"Avg Val Acc: {avg_acc:.4f} | \"\n",
    "          f\"LR: {optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "    if avg_acc > best_acc:\n",
    "        best_acc = avg_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(f\"  ✓ saved new best ({best_acc:.4f})\")\n",
    "\n",
    "print(f\"Done. Best Avg Val Acc = {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98930372-04ad-4878-a774-f99d0a5c8a46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
